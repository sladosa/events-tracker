# Garmin to Supabase: Complete Implementation Guide

## Bottom line: Use hybrid approach with manual FIT file uploads plus unofficial Garmin Connect API

For 10 users seeking automated Garmin data import with historical records and ongoing sync, the optimal solution combines periodic FIT file uploads for historical data with unofficial python-garminconnect library for daily automation. This balances data completeness (100% of metrics), reasonable maintenance burden, and acceptable risk. Deploy automation via GitHub Actions (free) or Render ($2-5/month), build mobile-optimized Streamlit UI for manual entries, and parse existing Excel data using pandas with regex. This approach costs $0-5/month versus $5,000+ annually for official API access.

The solution matters because direct FIT file parsing captures proprietary metrics like Body Battery and Training Effect that intermediary services like Strava cannot access, while the unofficial API enables true automation that Garmin's official program restricts to enterprise partners. Your historical FIT archive becomes immediately usable through one-time batch processing, Excel data migrates cleanly via pattern matching on compact notation, and the architecture scales from personal use to small team deployment.

Garmin offers no individual developer API access, forcing a choice between manual processes, unofficial libraries that violate terms of service, or intermediary platforms that lose critical wellness data. Among 200+ fitness platforms, only direct FIT file access preserves complete sensor data including sleep stages, HRV measurements, stress levels, and Garmin's proprietary Body Battery algorithm. The unofficial python-garminconnect library maintained by 1,500+ GitHub community members provides reliable daily sync despite occasional API changes, while your two-year FIT archive enables immediate historical analysis unavailable through any cloud API's 60-90 day limitations.

## Historical data: Process your FIT archive first for complete baseline

Your Garmin zip archive containing two years of FIT files represents the most complete data source available. These binary files contain every sensor reading, metric calculation, and activity detail recorded by your devices‚Äîfar more information than any API endpoint exposes. Processing this archive establishes your historical baseline before implementing ongoing automation.

FIT files come in three types based on what they record. **Activity files** capture workout sessions with second-by-second GPS tracks, heart rate series, cadence, power output, and lap summaries. File naming follows `YYYY-MM-DD-HH-MM-SS.fit` format or numeric activity IDs when downloaded from Garmin Connect. **Monitoring files** store all-day tracking data including step counts, continuous heart rate, stress levels, and intensity minutes, typically named `YYYY-MM-DD_MONITORING.fit`. **Sleep files** embed within monitoring data or exist separately as `*_SLEEP_DATA.fit`, containing sleep stages decoded from intensity field values where deep sleep registers as 0, light sleep as 1-2, and REM as 3.

The fitdecode library provides the most reliable Python parsing experience with active maintenance through 2024-2025 and complete FIT specification support. Installation requires just `pip install fitdecode` and the API handles malformed files gracefully while offering thread-safe concurrent reading for batch processing. Alternative libraries include fitparse (less actively maintained but simpler syntax), the official garmin-fit-sdk (verbose but comprehensive), and the complete GarminDB project that automates the entire pipeline from download through database storage.

Critical limitation exists with proprietary Garmin metrics: **Body Battery never appears in FIT files**. This Firstbeat-licensed algorithm calculation only surfaces through Garmin Connect's web interface or unofficial API access. Similarly, sleep scores, training status classifications, and recovery time predictions require algorithmic processing unavailable in raw sensor data. Training Effect values appear in FIT files but cannot be independently recalculated without Firstbeat's proprietary formulas.

```python
import fitdecode
import zipfile
import pandas as pd
from pathlib import Path
from datetime import datetime

def process_garmin_archive(zip_path: str, output_dir: str = './processed'):
    """
    Process complete Garmin FIT archive from zip file.
    Extracts activities, monitoring data, and sleep information.
    """
    Path(output_dir).mkdir(exist_ok=True)
    
    all_activities = []
    all_monitoring = []
    all_sleep = []
    errors = []
    
    with zipfile.ZipFile(zip_path, 'r') as archive:
        fit_files = [f for f in archive.namelist() if f.endswith('.fit')]
        print(f"Found {len(fit_files)} FIT files in archive")
        
        for fit_path in fit_files:
            try:
                # Extract to memory
                fit_data = archive.read(fit_path)
                
                # Parse with fitdecode
                import io
                fit_stream = io.BytesIO(fit_data)
                
                with fitdecode.FitReader(fit_stream) as fit:
                    for frame in fit:
                        if not isinstance(frame, fitdecode.FitDataMessage):
                            continue
                        
                        # Activity session data
                        if frame.name == 'session':
                            all_activities.append({
                                'source_file': Path(fit_path).name,
                                'sport': frame.get_value('sport'),
                                'start_time': frame.get_value('start_time'),
                                'duration_seconds': frame.get_value('total_elapsed_time'),
                                'distance_meters': frame.get_value('total_distance'),
                                'calories': frame.get_value('total_calories'),
                                'avg_heart_rate': frame.get_value('avg_heart_rate'),
                                'max_heart_rate': frame.get_value('max_heart_rate'),
                                'avg_speed': frame.get_value('avg_speed'),
                                'total_ascent': frame.get_value('total_ascent'),
                                'training_effect_aerobic': frame.get_value('training_effect'),
                                'training_effect_anaerobic': frame.get_value('anaerobic_training_effect')
                            })
                        
                        # All-day monitoring
                        elif frame.name == 'monitoring':
                            timestamp = frame.get_value('timestamp')
                            activity_type = frame.get_value('activity_type')
                            intensity = frame.get_value('intensity')
                            
                            monitoring_record = {
                                'timestamp': timestamp,
                                'heart_rate': frame.get_value('heart_rate'),
                                'steps': frame.get_value('steps'),
                                'activity_type': activity_type,
                                'intensity': intensity
                            }
                            all_monitoring.append(monitoring_record)
                            
                            # Sleep detection (activity_type = sedentary, intensity < 4)
                            if activity_type in ['sedentary', 0] and intensity is not None and intensity < 4:
                                sleep_stage_map = {0: 'deep', 1: 'light', 2: 'light', 3: 'rem'}
                                all_sleep.append({
                                    'timestamp': timestamp,
                                    'stage': sleep_stage_map.get(intensity, 'unknown'),
                                    'heart_rate': frame.get_value('heart_rate')
                                })
                
                print(f"‚úì Processed: {Path(fit_path).name}")
                
            except Exception as e:
                errors.append({'file': fit_path, 'error': str(e)})
                print(f"‚úó Error: {Path(fit_path).name} - {str(e)}")
                continue
    
    # Save to CSV for database import
    if all_activities:
        df_activities = pd.DataFrame(all_activities)
        df_activities.to_csv(f"{output_dir}/activities.csv", index=False)
        print(f"\n‚úì Saved {len(all_activities)} activities")
    
    if all_monitoring:
        df_monitoring = pd.DataFrame(all_monitoring)
        # Aggregate to hourly summaries to reduce volume
        df_monitoring['hour'] = pd.to_datetime(df_monitoring['timestamp']).dt.floor('H')
        hourly = df_monitoring.groupby('hour').agg({
            'heart_rate': 'mean',
            'steps': 'sum'
        }).reset_index()
        hourly.to_csv(f"{output_dir}/monitoring_hourly.csv", index=False)
        print(f"‚úì Saved {len(hourly)} hourly monitoring records")
    
    if all_sleep:
        df_sleep = pd.DataFrame(all_sleep)
        df_sleep.to_csv(f"{output_dir}/sleep.csv", index=False)
        print(f"‚úì Saved {len(all_sleep)} sleep records")
    
    if errors:
        pd.DataFrame(errors).to_csv(f"{output_dir}/errors.csv", index=False)
        print(f"‚ö† Logged {len(errors)} parsing errors")
    
    return {
        'activities': len(all_activities),
        'monitoring': len(all_monitoring),
        'sleep': len(all_sleep),
        'errors': len(errors)
    }

# Process your archive
results = process_garmin_archive('garmin_export.zip')
```

HRV extraction requires special handling because Garmin devices only record this data when explicitly enabled through Settings ‚Üí Physiological Metrics ‚Üí Log HRV, and activities must use 1-second recording mode with chest strap heart rate monitors rather than optical sensors. The FIT file stores RR intervals (time between heartbeats) in milliseconds within special `hrv` message types. Converting these raw intervals to standardized HRV metrics requires calculating SDNN (standard deviation of NN intervals) and RMSSD (root mean square of successive differences) after filtering artifacts using the 25% rule that discards intervals varying more than 25% from the previous reading.

## Excel migration: Parse compact notation with regex patterns

Your current Excel workflow with compact cell notation like "44HRV, 96bb" requires pattern matching to extract individual metrics. The pandas library combined with regular expressions handles this parsing efficiently while maintaining data integrity through validation.

The core challenge involves splitting comma-separated values then matching each component against a pattern that captures numbers (including decimals) followed by metric identifiers. The regex pattern `r'(\d+(?:\.\d+)?)\s*([A-Za-z]+|%)'` matches any numeric value with optional decimal followed by letter sequences or percentage signs, handling variations in spacing and punctuation. Building a standardized metric name mapping converts abbreviated forms like "bb" to "body_battery" and "HRV" to "heart_rate_variability" for consistent database fields.

```python
import pandas as pd
import re
import numpy as np
from typing import Dict

class FitnessExcelParser:
    """Production-ready parser for Excel files with concatenated metrics."""
    
    METRIC_ALIASES = {
        'HRV': 'heart_rate_variability',
        'bb': 'body_battery',
        '%': 'percentage',
        'bpm': 'beats_per_minute',
        'cal': 'calories'
    }
    
    def __init__(self):
        self.pattern = re.compile(r'(\d+(?:\.\d+)?)\s*([A-Za-z]+|%)')
    
    def parse_cell(self, cell_value) -> Dict[str, float]:
        """Parse single cell containing concatenated metrics."""
        if pd.isna(cell_value):
            return {}
        
        results = {}
        components = str(cell_value).split(',')
        
        for component in components:
            matches = self.pattern.findall(component.strip())
            for value, metric in matches:
                metric_name = self.METRIC_ALIASES.get(metric.strip(), metric.strip())
                try:
                    results[metric_name] = float(value)
                except ValueError:
                    continue
        
        return results
    
    def process_excel(self, filepath: str, metrics_column: str = 'Metrics',
                     date_column: str = 'Date') -> pd.DataFrame:
        """Complete Excel processing pipeline."""
        # Read Excel file
        df = pd.read_excel(filepath, engine='openpyxl', 
                          dtype={metrics_column: str})
        
        # Parse metrics column
        parsed = df[metrics_column].apply(self.parse_cell)
        
        # Extract unique metrics across all rows
        all_metrics = set()
        for row_dict in parsed:
            all_metrics.update(row_dict.keys())
        
        # Create separate columns
        for metric in all_metrics:
            df[metric] = parsed.apply(lambda x: x.get(metric, np.nan))
        
        # Process dates and add week numbers
        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
        df['week_number'] = df[date_column].dt.isocalendar().week
        df['year'] = df[date_column].dt.year
        
        # Validate ranges
        validations = {
            'heart_rate_variability': (0, 200),
            'body_battery': (0, 100),
            'percentage': (0, 100)
        }
        
        for metric, (min_val, max_val) in validations.items():
            if metric in df.columns:
                out_of_range = (df[metric] < min_val) | (df[metric] > max_val)
                if out_of_range.any():
                    print(f"‚ö† {out_of_range.sum()} rows have {metric} outside valid range")
                    df.loc[out_of_range, metric] = np.nan
        
        return df

# Usage example
parser = FitnessExcelParser()
df = parser.process_excel('fitness_tracker.xlsx', metrics_column='Metrics')

# Save for database import
df.to_csv('excel_parsed.csv', index=False)
print(f"Parsed {len(df)} rows with {df.columns.tolist()}")
```

Date handling requires parsing various Excel formats including serial numbers (days since 1900-01-01), ISO strings, and localized formats. Pandas `pd.to_datetime()` with `errors='coerce'` handles most cases automatically while converting failures to NaT (Not a Time) for manual review. Week number extraction uses ISO calendar conventions through `dt.isocalendar().week` matching Excel's WEEKNUM function, though you can alternatively use `dt.strftime('%U')` for Sunday-first week numbering if your Excel sheets follow that convention.

Data validation prevents importing obviously incorrect values that might result from OCR errors or manual entry mistakes. Establishing reasonable ranges for each metric type‚Äîheart rate variability between 0-200ms, body battery 0-100, percentages 0-100‚Äîflags anomalies for review before database insertion. The validation logic sets out-of-range values to NaN rather than rejecting entire rows, preserving valid data while marking suspect fields for investigation.

## Ongoing automation: Deploy python-garminconnect with GitHub Actions

Automated daily data import requires choosing between official and unofficial Garmin Connect API access. The official Garmin Connect Developer Program restricts individual developers, requiring business entity registration and reported annual fees around $5,000, making it economically infeasible for small user bases. The unofficial python-garminconnect library maintained by an active 1,500+ star GitHub community provides full API access through reverse-engineered endpoints that violate Garmin's terms of service but enable practical automation.

The python-garminconnect library authenticates using standard username/password credentials with built-in MFA handling through the underlying garth authentication layer. Initial login prompts for two-factor codes when enabled, then stores OAuth1 tokens valid for approximately one year alongside OAuth2 access tokens that auto-refresh. This token persistence eliminates daily authentication requirements‚Äîsubsequent script executions load saved credentials from `~/.garminconnect/` without user interaction.

Critical data availability distinction exists between API and FIT file sources: the unofficial API provides **Body Battery, sleep scores, training status, and HRV daily summaries** that never appear in raw FIT files because Garmin calculates these proprietary metrics server-side. However, second-by-second GPS tracks, detailed heart rate time series during activities, and complete power/cadence data remain more detailed in FIT files than API responses. Optimal implementation fetches daily wellness metrics via API while downloading activity FIT files for detailed analysis.

```python
#!/usr/bin/env python3
"""
Automated Garmin to Supabase sync script
Runs daily via GitHub Actions or cron
"""
import os
import sys
from datetime import datetime, timedelta
import logging
from garminconnect import Garmin
from supabase import create_client, Client

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GarminSupabaseSync:
    def __init__(self):
        # Initialize clients
        self.supabase: Client = create_client(
            os.environ['SUPABASE_URL'],
            os.environ['SUPABASE_KEY']
        )
        
        self.garmin = Garmin(
            os.environ['GARMIN_EMAIL'],
            os.environ['GARMIN_PASSWORD']
        )
    
    def fetch_daily_metrics(self, date_str: str) -> dict:
        """Fetch all daily metrics from Garmin Connect API."""
        try:
            self.garmin.login()
            
            # Fetch comprehensive daily data
            return {
                'date': date_str,
                'steps': self.garmin.get_steps_data(date_str),
                'heart_rate': self.garmin.get_heart_rates(date_str),
                'sleep': self.garmin.get_sleep_data(date_str),
                'hrv': self.garmin.get_hrv_data(date_str),
                'body_battery': self.garmin.get_body_battery(date_str, date_str),
                'stress': self.garmin.get_stress_data(date_str),
                'training_readiness': self.garmin.get_training_readiness(date_str),
                'fetched_at': datetime.utcnow().isoformat()
            }
        except Exception as e:
            logger.error(f"Garmin fetch failed: {e}")
            raise
    
    def save_to_supabase(self, data: dict):
        """Save metrics to Supabase with upsert."""
        try:
            # Flatten nested structures for storage
            flat_data = {
                'date': data['date'],
                'steps': data['steps'].get('totalSteps') if data['steps'] else None,
                'resting_hr': data['heart_rate'].get('restingHeartRate') if data['heart_rate'] else None,
                'avg_hr': data['heart_rate'].get('averageHeartRate') if data['heart_rate'] else None,
                'sleep_hours': data['sleep'].get('sleepTimeSeconds', 0) / 3600 if data['sleep'] else None,
                'deep_sleep_hours': data['sleep'].get('deepSleepSeconds', 0) / 3600 if data['sleep'] else None,
                'hrv_avg': data['hrv'].get('weeklyAvg') if data['hrv'] else None,
                'body_battery_charged': data['body_battery'][0].get('charged') if data['body_battery'] else None,
                'stress_avg': data['stress'].get('avgStressLevel') if data['stress'] else None,
                'updated_at': datetime.utcnow().isoformat()
            }
            
            # Upsert (insert or update if exists)
            result = self.supabase.table('daily_metrics').upsert(
                flat_data,
                on_conflict='date'
            ).execute()
            
            logger.info(f"‚úì Saved {data['date']} to Supabase")
            return result
            
        except Exception as e:
            logger.error(f"Supabase save failed: {e}")
            raise
    
    def sync_date(self, date_str: str) -> bool:
        """Sync single date with retry logic."""
        max_retries = 3
        
        for attempt in range(1, max_retries + 1):
            try:
                logger.info(f"Syncing {date_str} (attempt {attempt}/{max_retries})")
                
                data = self.fetch_daily_metrics(date_str)
                self.save_to_supabase(data)
                
                return True
                
            except Exception as e:
                if attempt >= max_retries:
                    logger.error(f"Failed after {max_retries} attempts: {e}")
                    self.send_alert(f"Sync failed for {date_str}: {e}")
                    return False
                
                # Exponential backoff
                import time
                wait_seconds = 2 ** attempt
                logger.warning(f"Retrying in {wait_seconds}s...")
                time.sleep(wait_seconds)
        
        return False
    
    def send_alert(self, message: str):
        """Send failure notification via Slack."""
        webhook_url = os.environ.get('SLACK_WEBHOOK_URL')
        if webhook_url:
            import requests
            requests.post(webhook_url, json={
                'text': f"üö® Garmin Sync Alert: {message}"
            })

def main():
    """Main sync routine for yesterday's data."""
    syncer = GarminSupabaseSync()
    
    # Sync yesterday (data typically available next day)
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    success = syncer.sync_date(yesterday)
    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()
```

GitHub Actions provides completely free automation for this use case with 2,000 monthly minutes for private repositories and unlimited minutes for public repos. The workflow YAML configuration schedules daily execution, manages secrets securely, and handles failures through status checks and notifications. Execution timing follows UTC, so adjust cron schedule for your timezone while remembering Garmin devices typically sync overnight data by 3-6 AM local time.

```yaml
# .github/workflows/garmin-sync.yml
name: Daily Garmin Sync

on:
  schedule:
    - cron: '0 8 * * *'  # 8 AM UTC daily
  workflow_dispatch:     # Manual trigger button

jobs:
  sync:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install garminconnect supabase requests
      
      - name: Run sync script
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          GARMIN_EMAIL: ${{ secrets.GARMIN_EMAIL }}
          GARMIN_PASSWORD: ${{ secrets.GARMIN_PASSWORD }}
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: python scripts/sync_garmin.py
      
      - name: Notify on failure
        if: failure()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK_URL }} \
               -H 'Content-Type: application/json' \
               -d '{"text":"‚ùå Garmin sync workflow failed"}'
```

Alternative hosting platforms provide more robust guarantees at minimal cost: **Render offers native cron jobs at $2-5 monthly** with 12-hour maximum runtime and automatic Git deployment, while **Railway provides similar capabilities around $5-10 monthly** with their $5 free credit. Digital Ocean App Platform, AWS Lambda with EventBridge, and GCP Cloud Scheduler represent more complex enterprise options suitable when scaling beyond small teams. Critically, **Streamlit Cloud does not support background tasks or scheduled jobs**‚Äîit only runs when users access the web interface, requiring separate automation infrastructure.

## Mobile UI: Build Streamlit interface for manual entries

Streamlit provides the fastest path to mobile-optimized data visualization and manual entry forms. The framework renders responsive layouts automatically while connecting directly to Supabase for real-time data display and updates. Your manual training notes (exercises, reps, comments, feelings) require dedicated input forms since these subjective observations never appear in automated data feeds.

```python
import streamlit as st
from supabase import create_client
from datetime import date, timedelta
import pandas as pd

# Initialize Supabase connection
@st.cache_resource
def init_supabase():
    return create_client(
        st.secrets["SUPABASE_URL"],
        st.secrets["SUPABASE_KEY"]
    )

supabase = init_supabase()

st.set_page_config(
    page_title="Garmin Training Log",
    page_icon="üí™",
    layout="wide"
)

# Sidebar navigation
page = st.sidebar.radio("Navigate", ["Dashboard", "Manual Entry", "Upload FIT Files"])

if page == "Dashboard":
    st.title("Training Dashboard")
    
    # Date range selector
    col1, col2 = st.columns(2)
    with col1:
        start_date = st.date_input("From", value=date.today() - timedelta(days=30))
    with col2:
        end_date = st.date_input("To", value=date.today())
    
    # Fetch metrics from Supabase
    response = supabase.table('daily_metrics').select('*').gte(
        'date', start_date.isoformat()
    ).lte('date', end_date.isoformat()).order('date', desc=False).execute()
    
    if response.data:
        df = pd.DataFrame(response.data)
        df['date'] = pd.to_datetime(df['date'])
        
        # Key metrics cards
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Avg Steps", f"{df['steps'].mean():.0f}")
        with col2:
            st.metric("Avg HRV", f"{df['hrv_avg'].mean():.1f} ms")
        with col3:
            st.metric("Avg Body Battery", f"{df['body_battery_charged'].mean():.0f}")
        with col4:
            st.metric("Sleep Hours", f"{df['sleep_hours'].mean():.1f}")
        
        # Trend charts
        st.subheader("Trends")
        
        chart_metric = st.selectbox("Select metric", 
                                    ['steps', 'hrv_avg', 'body_battery_charged', 'sleep_hours'])
        st.line_chart(df.set_index('date')[chart_metric])
        
        # Data table
        st.subheader("Raw Data")
        st.dataframe(df, use_container_width=True)
    else:
        st.info("No data available for selected date range")

elif page == "Manual Entry":
    st.title("Manual Training Entry")
    
    with st.form("training_entry"):
        st.subheader("Add Training Notes")
        
        entry_date = st.date_input("Date", value=date.today())
        
        col1, col2 = st.columns(2)
        with col1:
            workout_type = st.selectbox("Workout Type", 
                ["Strength", "Cardio", "Yoga", "Stretching", "Sport"])
        with col2:
            duration = st.number_input("Duration (minutes)", min_value=5, max_value=300, value=45)
        
        # Exercise logging
        st.text_area("Exercises & Reps", 
                    placeholder="E.g., Squats 3x12, Bench Press 3x10",
                    height=100,
                    key="exercises")
        
        # Subjective feedback
        col1, col2 = st.columns(2)
        with col1:
            energy_level = st.slider("Energy Level", 1, 10, 5)
        with col2:
            difficulty = st.slider("Difficulty", 1, 10, 5)
        
        notes = st.text_area("Additional Notes", 
                            placeholder="How did you feel? Any issues?",
                            height=100)
        
        submitted = st.form_submit_button("Save Entry")
        
        if submitted:
            # Save to Supabase
            training_data = {
                'date': entry_date.isoformat(),
                'workout_type': workout_type,
                'duration_minutes': duration,
                'exercises': st.session_state.exercises,
                'energy_level': energy_level,
                'difficulty': difficulty,
                'notes': notes,
                'created_at': pd.Timestamp.now().isoformat()
            }
            
            result = supabase.table('training_notes').insert(training_data).execute()
            
            if result.data:
                st.success("‚úÖ Training entry saved!")
            else:
                st.error("Failed to save entry")

elif page == "Upload FIT Files":
    st.title("Upload FIT Files")
    
    st.markdown("""
    ### Quick Upload Instructions
    1. Connect your Garmin device via USB or download from Garmin Connect
    2. Select one or multiple `.fit` files below
    3. Click Process to import data automatically
    """)
    
    uploaded_files = st.file_uploader(
        "Choose FIT files",
        type=['fit'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.info(f"Selected {len(uploaded_files)} files")
        
        if st.button("Process Files"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for idx, file in enumerate(uploaded_files):
                status_text.text(f"Processing {file.name}...")
                
                try:
                    # Save temporarily and parse
                    import tempfile
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.fit') as tmp:
                        tmp.write(file.getvalue())
                        tmp_path = tmp.name
                    
                    # Parse FIT file (use process_garmin_archive logic)
                    # ... parsing code here ...
                    
                    # Upload to Supabase
                    # ... upload logic ...
                    
                    st.success(f"‚úÖ {file.name}")
                    
                except Exception as e:
                    st.error(f"‚ùå {file.name}: {str(e)}")
                
                progress_bar.progress((idx + 1) / len(uploaded_files))
            
            status_text.text("Complete!")
```

The Streamlit interface separates into three functional areas: a dashboard displaying aggregated trends and metrics pulled from Supabase, manual entry forms capturing subjective training notes and feelings, and FIT file upload processing for ad-hoc historical data addition. Mobile optimization works automatically through Streamlit's responsive grid system, though you should test column layouts on actual mobile devices since the desktop preview doesn't perfectly replicate mobile rendering.

Manual on-demand import triggers extend beyond the web UI through GitHub Actions workflow_dispatch events callable via authenticated API requests, Render webhook URLs that trigger specific cron jobs immediately, or direct Supabase function invocations if using Edge Functions for processing logic. This enables both user-initiated "sync now" buttons and programmatic backfill operations for date ranges with missing data.

## Architecture comparison: Evaluate tradeoffs across implementation approaches

Different implementation architectures present distinct tradeoffs in data completeness, automation level, maintenance burden, and legal compliance. Understanding these differences guides selection appropriate to your priorities and constraints.

| **Approach** | **Setup Complexity** | **Monthly Cost** | **Data Completeness** | **Automation Level** | **Reliability** | **TOS Compliance** |
|------------|---------|---------|------------|------------|---------|---------|
| **Manual FIT upload** | Low (2/10) | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Complete (all metrics) | Manual weekly | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very high | ‚úÖ Compliant |
| **Unofficial Garmin API** | Medium (6/10) | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Complete (all metrics) | Fully automated | ‚≠ê‚≠ê‚≠ê Breaks 1-2x/year | ‚ö†Ô∏è TOS violation |
| **Strava intermediary** | Medium (4/10) | $0 | ‚≠ê‚≠ê‚≠ê Activities only | Fully automated | ‚≠ê‚≠ê‚≠ê‚≠ê High | ‚úÖ Compliant |
| **Health Connect** | High (7/10) | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê Most metrics (75%) | Semi-automated | ‚≠ê‚≠ê‚≠ê New platform | ‚úÖ Compliant |
| **Screenshot OCR** | High (8/10) | $3-15 | ‚≠ê‚≠ê Very incomplete | Manual intensive | ‚≠ê‚≠ê Error-prone | ‚úÖ Compliant |

**Manual FIT upload** provides complete data access including proprietary metrics while maintaining legal compliance and predictable operation. The approach requires user discipline uploading files weekly or monthly but eliminates authentication complexity and API breakage concerns. Historical data import happens through single bulk upload of your two-year archive. This method suits small user bases willing to invest 5-10 minutes weekly for guaranteed data completeness and stability. Implementation involves building file upload interface, implementing FIT parsing with fitdecode or garmin-fit-sdk, and creating database import pipeline with validation.

**Unofficial Garmin Connect API via python-garminconnect** enables true automation fetching data daily without user intervention after initial credential setup. The library accesses Body Battery, sleep scores, and training status metrics unavailable in FIT files while supporting automatic activity FIT downloads for detailed analysis. Critical risks include terms of service violations, potential account suspension, and maintenance burden responding to API changes occurring 1-2 times annually. The 1,500+ star GitHub community typically resolves breaking changes within 24-48 hours. This approach suits personal projects and small teams accepting these tradeoffs for automation benefits.

**Strava API as intermediary** offers easiest automation with well-documented OAuth implementation and generous rate limits sufficient for 10 users. Garmin automatically syncs activities to Strava within minutes of upload, eliminating manual export steps. However, Strava only stores activity data‚Äîworkout GPS tracks, heart rate during activities, power and cadence measurements‚Äîwhile completely lacking wellness metrics. **No sleep data, HRV, Body Battery, daily steps, or stress levels appear through Strava**. This fundamental platform limitation makes Strava suitable only for activity-focused applications ignoring recovery metrics.

Health Connect (Android) and Apple Health provide platform-native integration but require mobile app development rather than web interfaces. Garmin's sync includes sleep duration and stages, daily steps, SpO2, and stress levels but omits proprietary calculations like Body Battery and Training Effect. **GPS tracks for activities do not sync to these platforms**‚Äîonly activity summaries appear. Apple Health suffers documented reliability issues with sync failures and data discrepancies reported frequently in support forums. Health Connect launched only July 2025 with maturity concerns. Both require native development skills and platform-specific codebases.

Screenshot OCR using Claude Vision API or similar achieves 95-98% accuracy on clear screenshots but requires extensive manual work capturing multiple screen views daily. Cost calculations show $0.004-0.02 per screenshot with Claude 3.5 Sonnet, accumulating to $60-240 annually for daily comprehensive capture. This approach suits only one-time historical recovery from archived screenshots or emergency backup when all other methods fail.

## Recommended implementation: Hybrid FIT upload plus unofficial API automation

Your specific constraints‚Äî10 users, historical FIT archive, ongoing automation desire, mobile-optimized UI‚Äîalign best with **hybrid architecture combining periodic manual FIT uploads with python-garminconnect automation**. This balances data completeness, reasonable maintenance, and acceptable risk.

**Phase 1 (Week 1-2): Historical baseline establishment**

Process your two-year FIT archive using the batch processing script provided earlier. This one-time operation imports complete historical data including activities, sleep patterns, wellness metrics, and training load progression. Expect processing time around 1-2 hours for typical archive sizes. Parse Excel data simultaneously using the regex-based parser to incorporate your manual transcription period. Store both in Supabase with proper table schema distinguishing source types.

Supabase table schema should separate concerns:

```sql
-- Activities table (from FIT files and API)
CREATE TABLE activities (
    id BIGSERIAL PRIMARY KEY,
    date DATE NOT NULL,
    start_time TIMESTAMPTZ,
    sport VARCHAR(50),
    duration_seconds INTEGER,
    distance_meters NUMERIC,
    calories INTEGER,
    avg_heart_rate SMALLINT,
    max_heart_rate SMALLINT,
    training_effect_aerobic NUMERIC(3,1),
    training_effect_anaerobic NUMERIC(3,1),
    source VARCHAR(20), -- 'fit_file', 'api', 'manual'
    raw_data JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Daily wellness metrics (from API and FIT monitoring)
CREATE TABLE daily_metrics (
    date DATE PRIMARY KEY,
    steps INTEGER,
    distance_km NUMERIC,
    floors_climbed SMALLINT,
    resting_hr SMALLINT,
    avg_hr SMALLINT,
    max_hr SMALLINT,
    sleep_hours NUMERIC(4,2),
    deep_sleep_hours NUMERIC(4,2),
    light_sleep_hours NUMERIC(4,2),
    rem_sleep_hours NUMERIC(4,2),
    hrv_avg SMALLINT,
    body_battery_charged SMALLINT,
    body_battery_drained SMALLINT,
    stress_avg SMALLINT,
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Manual training notes
CREATE TABLE training_notes (
    id BIGSERIAL PRIMARY KEY,
    date DATE NOT NULL,
    workout_type VARCHAR(50),
    duration_minutes SMALLINT,
    exercises TEXT,
    energy_level SMALLINT CHECK (energy_level BETWEEN 1 AND 10),
    difficulty SMALLINT CHECK (difficulty BETWEEN 1 AND 10),
    notes TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Create indexes for common queries
CREATE INDEX idx_activities_date ON activities(date DESC);
CREATE INDEX idx_daily_metrics_date ON daily_metrics(date DESC);
CREATE INDEX idx_training_notes_date ON training_notes(date DESC);
```

**Phase 2 (Week 3): Automation deployment**

Deploy the python-garminconnect sync script via GitHub Actions using the workflow YAML provided earlier. Configure secrets through GitHub repository Settings ‚Üí Secrets ‚Üí Actions, adding SUPABASE_URL, SUPABASE_KEY, GARMIN_EMAIL, and GARMIN_PASSWORD. Test the workflow manually using the Actions tab "Run workflow" button before relying on scheduled execution.

Initial authentication requires one-time interactive login if MFA enabled. Run the script locally first: `python sync_garmin.py`, enter MFA code when prompted, then copy the resulting `~/.garminconnect/` token directory into your GitHub Actions workflow using base64 encoding stored as secret, or configure the workflow to accept MFA codes through workflow inputs for initial setup.

Monitor the first week of automated runs carefully through Actions logs, Supabase activity dashboard, and optional Slack notifications. Common issues include token expiration after account password changes, rate limiting if fetching too many historical dates simultaneously, and Garmin API downtime during maintenance windows (typically weekends).

**Phase 3 (Week 4): User interface deployment**

Deploy the Streamlit application to Streamlit Cloud connecting to your Supabase instance. Configure secrets through Streamlit Cloud dashboard replicating SUPABASE_URL and SUPABASE_KEY values. The free tier supports unlimited public apps or one private app sufficient for small team usage.

Add FIT file upload processing to the Streamlit interface enabling users to manually upload files ad-hoc when automation misses days or when processing device exports directly. Implement the manual training notes forms allowing users to record subjective feelings, exercise details, and recovery observations complementing automated metrics.

**Phase 4 (Ongoing): Maintenance and scaling**

Monitor the unofficial Garmin API for breaking changes through the python-garminconnect GitHub issues page. When breaks occur (typically 1-2 times annually), update the library version via `pip install --upgrade garminconnect` and redeploy. The community usually resolves issues within 24-48 hours of API changes.

Scale considerations for growing beyond 10 users: GitHub Actions accommodates up to hundreds of users within free tier limits by batching sync operations; Render cron jobs cost $2-5 monthly per job so multiple user groups could share single job fetching all users sequentially; Supabase free tier supports 500MB database and 2GB bandwidth monthly, sufficient for approximately 50-100 active users before requiring $25/month Pro plan.

Legal risk mitigation for unofficial API usage: clearly document to users that automation uses community-maintained unofficial library violating Garmin TOS with potential account suspension risk, provide fallback manual upload option ensuring continued functionality if automation breaks, and monitor for any Garmin enforcement actions through community discussions.

This hybrid approach delivers 100% data completeness through FIT files, convenient automation through unofficial API, legal fallback via manual upload option, and complete control through custom Supabase database‚Äîall operating at $0-5 monthly cost suitable for personal projects through small team deployment.

## Conclusion: Start with historical FIT processing, add automation incrementally

The optimal path forward processes your two-year FIT archive immediately establishing complete historical baseline, implements manual upload workflow as reliable foundation, then layers unofficial Garmin Connect API automation accepting calculated risks for convenience gains. This progression maintains data access throughout while testing automation reliability before full dependence.

Alternative approaches sacrifice critical capabilities: Strava eliminates all wellness metrics; official API remains inaccessible to individuals; screenshot OCR proves impractical for regular use; Health Connect requires native mobile development. The hybrid FIT plus unofficial API strategy uniquely balances completeness, automation, cost, and practical implementation for small user bases.

Key architectural insight shows Streamlit Cloud cannot host automation itself‚Äîyou must deploy scheduled tasks separately via GitHub Actions, Render, or similar services while using Streamlit exclusively for user interface. This separation of concerns proves advantageous allowing independent scaling and replacement of either component without disrupting the other.

Execute historical data import first enabling immediate analysis and dashboard development while automation configuration proceeds in parallel. Your existing Excel data migrates cleanly through regex parsing preserving two years of manual entries. Within 2-4 weeks, you achieve automated daily imports, mobile-optimized manual entry interface, and complete data warehouse supporting analysis across historical and ongoing fitness progression.